"""Copy of RAGgers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11x8SUtlOrM7WR18JEWdQfJtxeK91ozUc

# Installations
"""

"""# Imports"""

import os
import faiss
import json
import numpy as np
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import pipeline
import pdfplumber
import docx
from email import policy
from email.parser import BytesParser
from groq import Groq
import re
from langchain_core.runnables import RunnableLambda
from langchain_core.tools import Tool
from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.prompts import PromptTemplate

"""# Main Code"""

# 1. Document Loading & Chunking

def extract_text_from_pdf(path: str) -> List[Dict]:
    chunks = []
    with pdfplumber.open(path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                chunks.append((text.strip(), i + 1))
    return chunks

def extract_text_from_docx(path: str) -> List[Dict]:
    doc = docx.Document(path)
    full_text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    return [(full_text, 1)]

def extract_text_from_email(path: str) -> List[Dict]:
    with open(path, 'rb') as f:
        msg = BytesParser(policy=policy.default).parse(f)
    body = msg.get_body(preferencelist=('plain')).get_content()
    return [(body.strip(), 1)]

def load_and_chunk_document(path: str) -> List[Dict]:
    ext = os.path.splitext(path)[1].lower()
    if ext == ".pdf":
        raw_chunks = extract_text_from_pdf(path)
    elif ext == ".docx":
        raw_chunks = extract_text_from_docx(path)
    elif ext in [".eml", ".email"]:
        raw_chunks = extract_text_from_email(path)
    else:
        raise ValueError("Unsupported file format")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    all_chunks = []
    for text, page in raw_chunks:
        split = text_splitter.split_text(text)
        for chunk in split:
            all_chunks.append({
                "content": chunk,
                "page": page,
                "source_file": os.path.basename(path)
            })
    return all_chunks

# 2. Embedding + FAISS Indexing

def build_faiss_index(chunks: List[Dict], model_name='all-MiniLM-L6-v2'):
    model = SentenceTransformer(model_name)
    texts = [c['content'] for c in chunks]
    embeddings = model.encode(texts)

    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings).astype('float32'))

    return index, embeddings, model

# 3. Hybrid Semantic Retrieval

def search_top_chunks(query: str, chunks: List[Dict], model, index, k=5) -> List[Dict]:
    query_vec = model.encode([query])
    D, I = index.search(np.array(query_vec).astype('float32'), k)
    return [chunks[i] for i in I[0]]

# 4. Query Structuring (new)

def parse_query(query: str) -> Dict:
    result = {
        "age": re.findall(r"\d{2}(?=-year|[MF])", query),
        "gender": "M" if "M" in query.upper() else "F" if "F" in query.upper() else None,
        "procedure": re.findall(r"(?<=,\s).*surgery", query),
        "location": re.findall(r"in ([A-Za-z]+)", query),
        "policy_duration": re.findall(r"\d+-month", query)
    }
    return result

# 5. Prompt Template

prompt_template = PromptTemplate(
    input_variables=["query", "context"],
    template="""
You are an intelligent insurance assistant.

User has asked:
{query}

Refer to the following policy clauses to answer:
---
{context}
---

Important instructions:
- Do NOT assume any medical history or user behavior unless explicitly mentioned in the query.
- Do NOT confuse unrelated medical domains.
- Only use clauses that match the medical situation in the userâ€™s query.
- If a clause is not relevant, do not use it.

Your task:
- Return a decision: Approved / Rejected
- Estimated payout amount (if applicable)
- Justify based on which clause(s)
- Format the result in strict JSON with keys: "decision", "amount", "justification", "clause_reference"
"""
)

# 6. Call the LLM (Groq)

def run_inference(prompt: str, groq_client: Groq) -> Dict:
    try:
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama3-8b-8192",
        )
        result = chat_completion.choices[0].message.content
        json_match = result[result.find('{'): result.rfind('}')+1]
        return json.loads(json_match)
    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {e}")
        return {"error": "Could not parse output", "raw": result}
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return {"error": "An unexpected error occurred", "raw": str(e)}

# 7. Reflection Tool to Detect Assumptions

def detect_assumption(prompt: str) -> str:
    return f"Does this reasoning make assumptions not stated in the query?\n{prompt}\n\nBe critical and point them out explicitly."

def check_assumptions_wrapper(p):
    return run_inference(detect_assumption(p), groq_client)

reflection_tool = Tool(
    name="AssumptionChecker",
    func=check_assumptions_wrapper,
    description="Checks if the given reasoning makes unstated assumptions."
)

# 8. Agent Execution

def build_agent(groq_client: Groq):
    tools = [
        Tool(
            name="RetrieveRelevantChunks",
            func=lambda input: search_top_chunks(input['query'], input['chunks'], input['model'], input['index']),
            description="Retrieves relevant document clauses for the query"
        ),
        reflection_tool
    ]

    agent = create_react_agent(
        tools=tools,
        llm=lambda input: run_inference(prompt_template.format(query=input['query'], context="\n\n".join([f"[Clause - Pg {c['page']}] {c['content']}" for c in input['chunks']])), groq_client)
    )
    return AgentExecutor(agent=agent, verbose=True)

# 9. End-to-End Runner

def process_query(query: str, doc_path: str, groq_client: Groq):
    chunks = load_and_chunk_document(doc_path)
    index, embeddings, model = build_faiss_index(chunks)
    top_chunks = search_top_chunks(query, chunks, model, index)
    prompt = prompt_template.format(query=query, context="\n\n".join([f"[Clause - Pg {c['page']}] {c['content']}" for c in top_chunks]))
    response = run_inference(prompt, groq_client)
    return response

groq_client = Groq(api_key="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")

def ask_a_question(query, doc_path):
    result = process_query(query, doc_path, groq_client)
    print(json.dumps(result, indent=2))

"""# Testing"""

ask_a_question("46M, knee surgery, Pune, 3-month policy", "/content/BajajPolicies.pdf")

queries = ["21M, knee surgery, Pune, 3-month policy", "Can I get coverage for mental illness treatment if admitted to a psychiatric hospital?"]
for query in queries:
  ask_a_question(query, "/content/BajajPolicies.pdf")

queries = ["Is Ayurvedic treatment covered under hospitalization benefits?", "Do I need to pay anything if I use cashless facility in a network hospital for surgery?", "Am I eligible for health check-up after one year of policy renewal?", "Is ambulance cost covered for transferring a patient from one hospital to another?"]

for query in queries:
  ask_a_question(query, "/content/BajajPolicies.pdf")

ask_a_question("Is cataract operation included for a 58-year-old under this health policy from Bajaj?", "/content/BajajPolicies.pdf")

ask_a_question("It's been 2 months since I bought the policy. Can I claim for gallbladder removal surgery now?", "/content/BajajPolicies.pdf")

ask_a_question("Will the insurance cover treatment for alcohol-related liver damage?", "/content/BajajPolicies.pdf")

ask_a_question("Is a robotic knee replacement at Ruby Hall Clinic in Pune covered under this policy?", "/content/BajajPolicies.pdf")

"""## API ENDPOINT"""

