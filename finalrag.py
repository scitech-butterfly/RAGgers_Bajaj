# -*- coding: utf-8 -*-
"""finalrag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CEg_G3u2hkaQjKRGaCcTuS70W2w1Jf3g

# Installations
"""

"""# Imports"""

import faiss
import json
import numpy as np
from sentence_transformers import SentenceTransformer
from groq import Groq
from langchain_core.prompts import PromptTemplate
import pdfplumber
from langchain.text_splitter import RecursiveCharacterTextSplitter
from typing import Dict

"""# Main Code"""

def get_index_and_chunks(doc_path):
    """
    Loads chunks and builds a FAISS index from either:
    - a file-like object (e.g., BytesIO for streamed PDFs)
    - a local file path
    Uses a streaming generator to avoid loading the entire document into memory.
    """
    if hasattr(doc_path, "read"):  # file-like object
        gen = build_faiss_index_streaming_generator(doc_path)
    else:  # path string
        with open(doc_path, "rb") as f:
            gen = build_faiss_index_streaming_generator(f)

    chunks = []
    index = None
    model = None
    for chunk, idx, mdl in gen:
        chunks.append(chunk)
        index = idx
        model = mdl

    return chunks, index, model


# Streaming generator to build FAISS without storing all chunks in memory
def build_faiss_index_streaming_generator(file_obj, model_name='all-MiniLM-L6-v2', chunk_size=1000, chunk_overlap=200):
    """
    Streams through PDF pages, yields each chunk as it's embedded into FAISS.
    Keeps memory usage constant regardless of PDF size.
    """
    model = SentenceTransformer(model_name)
    dim = model.get_sentence_embedding_dimension()
    index = faiss.IndexFlatL2(dim)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )

    with pdfplumber.open(file_obj) as pdf:
        for page_num, page in enumerate(pdf.pages, start=1):
            text = page.extract_text()
            if not text:
                continue

            for chunk_content in text_splitter.split_text(text.strip()):
                embedding = model.encode([chunk_content])
                index.add(np.array(embedding).astype('float32'))

                # Yield one chunk at a time instead of storing all in memory
                yield {
                    "content": chunk_content,
                    "page": page_num
                }, index, model


# Process a query using prebuilt index, chunks, and model
def process_query_with_index(query, chunks, index, model, groq_client):
    """
    Runs semantic search on a prebuilt FAISS index and generates an answer.
    Avoids rebuilding the index for every question.
    """
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding).astype('float32'), k=5)

    retrieved_chunks = []
    for idx in indices[0]:
        if idx < len(chunks):
            retrieved_chunks.append(chunks[idx]['content'])

    context = "\n".join(retrieved_chunks)
    prompt = prompt_template.format(query=query, context=context)
    return run_inference(prompt, groq_client)

# Prompt Template
prompt_template = PromptTemplate(
    input_variables=["query", "context"],
    template="""
You are an intelligent insurance assistant.

User has asked:
{query}

Refer to the following policy clauses to answer:
---
{context}
---

Important instructions:
- Do NOT assume any medical history or user behavior unless explicitly mentioned in the query.
- Do NOT confuse unrelated medical domains.
- Only use clauses that match the medical situation in the userâ€™s query.
- If a clause is not relevant, do not use it.

Your task:
- Return a decision: Approved / Rejected
- Estimated payout amount (if applicable)
- Justify based on which clause(s)
- Format the result in strict JSON with keys: "decision", "amount", "justification", "clause_reference"
"""
)

# Call the LLM (Groq)
def run_inference(prompt: str, groq_client: Groq) -> Dict:
    try:
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama3-8b-8192",
        )
        result = chat_completion.choices[0].message.content.strip()
        json_match = result[result.find('{'): result.rfind('}')+1]
        return json.loads(json_match)
    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {e}")
        return {"error": "Could not parse output", "raw": result}
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return {"error": "An unexpected error occurred", "raw": str(e)}

groq_client = Groq(api_key="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")
